{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a class for the XOR_net\n",
    "class XOR_net:\n",
    "    def __init__(self, activation=\"sigmoid\", weights=None):\n",
    "        FUNC_TABLE = {\"sigmoid\": self.sigmoid, \"tanh\": self.tanh}\n",
    "        DFUNC_TABLE = {\"sigmoid\": self.dsigmoid, \"tanh\": self.dtanh}\n",
    "        self.weights = self.random_weights() if weights is None else weights\n",
    "        # Storage for skipping the calculation of some variables\n",
    "        # 0, 1: inputs of NN\n",
    "        # 2, 3: values of the hidden layer pre activation\n",
    "        # 4, 5: values of the hidden layer post activation\n",
    "        # 6: value of the final node pre activation\n",
    "        # 7: value of the final node post activation\n",
    "        self.store = np.zeros(8)\n",
    "        self.actf = FUNC_TABLE[activation]  # activation function\n",
    "        self.dactf = DFUNC_TABLE[activation]  # derivative of activation function\n",
    "        self.inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        self.outputs = np.array([0, 1, 1, 0])\n",
    "\n",
    "    def simulate(self, inpt):\n",
    "        self.store[:2] = inpt\n",
    "        self.store[2:4] = np.dot(self.weights[:6].reshape((2, 3)), np.append(inpt, 1))\n",
    "        self.store[4:6] = [self.actf(el) for el in self.store[2:4]]\n",
    "        self.store[6] = np.dot(self.weights[6:], np.append(self.store[4:6], 1))\n",
    "        self.store[7] = self.actf(self.store[6])\n",
    "        return self\n",
    "\n",
    "    def output(self, discrete=False):\n",
    "        if discrete:\n",
    "            return 1 if self.store[-1] >= 0.5 else 0\n",
    "        else:\n",
    "            return self.store[-1]\n",
    "\n",
    "    def mse(self):\n",
    "        return sum(\n",
    "            [\n",
    "                (self.simulate(inpt).output() - output) ** 2\n",
    "                for inpt, output in zip(self.inputs, self.outputs)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def grdmse(self):\n",
    "        par_derivs = np.zeros((9))\n",
    "        for inpt, output in zip(self.inputs, self.outputs):\n",
    "            self.simulate(inpt)\n",
    "            temp = 2 * (self.store[-1] - output) * self.dactf(self.store[6])\n",
    "\n",
    "            par_derivs[6:] += temp * np.append(self.store[4:6], 1)\n",
    "            par_derivs[3:6] += (\n",
    "                temp\n",
    "                * self.weights[7]\n",
    "                * self.dactf(self.store[3])\n",
    "                * np.append(self.store[:2], 1)\n",
    "            )\n",
    "            par_derivs[:3] += (\n",
    "                temp\n",
    "                * self.weights[6]\n",
    "                * self.dactf(self.store[2])\n",
    "                * np.append(self.store[:2], 1)\n",
    "            )\n",
    "        return par_derivs\n",
    "\n",
    "    def update_weights(self, eta=0.01):\n",
    "        self.weights -= eta * self.grdmse()\n",
    "\n",
    "    def random_weights(self):\n",
    "        weights = np.zeros((9))\n",
    "        weights[:6] = np.random.normal(0, np.sqrt(2 / (2 + 2)), (6))\n",
    "        weights[6:] = np.random.normal(0, np.sqrt(2 / (2 + 1)), (3))\n",
    "        return weights\n",
    "\n",
    "    def print_test(self, discrete=True):\n",
    "        for inpt, output in zip(self.inputs, self.outputs):\n",
    "            print(\n",
    "                f\"{inpt[0]} ^ {inpt[1]} = {self.simulate(inpt).output(discrete):{'.3f' if not discrete else ''}} [{output}]\"\n",
    "            )\n",
    "        print(f\"final mse: {self.mse()}\")\n",
    "\n",
    "    def is_xor(self):\n",
    "        for inpt, output in zip(self.inputs, self.outputs):\n",
    "            if self.simulate(inpt).output(True) != output:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @lru_cache()\n",
    "    def sigmoid(self, x):\n",
    "        return (1 + np.exp(-x)) ** -1\n",
    "\n",
    "    @lru_cache()\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    @lru_cache()\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @lru_cache()\n",
    "    def dtanh(self, x):\n",
    "        return np.cosh(x) ** -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ^ 0 = 0.487 [0]\n",
      "0 ^ 1 = 0.500 [1]\n",
      "1 ^ 0 = 0.503 [1]\n",
      "1 ^ 1 = 0.490 [0]\n",
      "final mse: 0.9742029814634141\n",
      "0 ^ 0 = 0.488 [0]\n",
      "0 ^ 1 = 0.501 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.470 [0]\n",
      "final mse: 0.9579193745211692\n",
      "0 ^ 0 = 0.498 [0]\n",
      "0 ^ 1 = 0.534 [1]\n",
      "1 ^ 0 = 0.522 [1]\n",
      "1 ^ 1 = 0.487 [0]\n",
      "final mse: 0.9304780147373188\n",
      "0 ^ 0 = 0.495 [0]\n",
      "0 ^ 1 = 0.509 [1]\n",
      "1 ^ 0 = 0.502 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.9842611107180899\n",
      "0 ^ 0 = 0.492 [0]\n",
      "0 ^ 1 = 0.506 [1]\n",
      "1 ^ 0 = 0.503 [1]\n",
      "1 ^ 1 = 0.492 [0]\n",
      "final mse: 0.9748141871508356\n",
      "0 ^ 0 = 0.497 [0]\n",
      "0 ^ 1 = 0.509 [1]\n",
      "1 ^ 0 = 0.509 [1]\n",
      "1 ^ 1 = 0.494 [0]\n",
      "final mse: 0.973072485966206\n",
      "0 ^ 0 = 0.486 [0]\n",
      "0 ^ 1 = 0.503 [1]\n",
      "1 ^ 0 = 0.504 [1]\n",
      "1 ^ 1 = 0.496 [0]\n",
      "final mse: 0.9754251036717647\n",
      "0 ^ 0 = 0.498 [0]\n",
      "0 ^ 1 = 0.502 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.498 [0]\n",
      "final mse: 0.9936765381556671\n",
      "0 ^ 0 = 0.496 [0]\n",
      "0 ^ 1 = 0.501 [1]\n",
      "1 ^ 0 = 0.500 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.994370681244225\n",
      "0 ^ 0 = 0.494 [0]\n",
      "0 ^ 1 = 0.505 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.9874435091055865\n",
      "Average number of iterations: 690570\n"
     ]
    }
   ],
   "source": [
    "n_total = []\n",
    "for i in range(10):\n",
    "    satisfied = False\n",
    "    n = 0\n",
    "    while not satisfied:\n",
    "        n += 1\n",
    "        nn = XOR_net()\n",
    "        satisfied = nn.is_xor()\n",
    "    nn.print_test(False)\n",
    "    n_total.append(n)\n",
    "print(f\"Average number of iterations: {int(np.mean(n_total))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: MSE = 1.8409\n",
      "Epoch 1000: MSE = 0.9376\n",
      "Epoch 2000: MSE = 0.7289\n",
      "Epoch 3000: MSE = 0.6926\n",
      "Epoch 4000: MSE = 0.6815\n",
      "Epoch 5000: MSE = 0.6757\n",
      "Epoch 6000: MSE = 0.6701\n",
      "Epoch 7000: MSE = 0.6050\n",
      "Epoch 8000: MSE = 0.0562\n",
      "Epoch 9000: MSE = 0.0200\n",
      "0 ^ 0 = 0 [0]\n",
      "0 ^ 1 = 1 [1]\n",
      "1 ^ 0 = 1 [1]\n",
      "1 ^ 1 = 0 [0]\n",
      "final mse: 0.011784508393869029\n",
      "Epoch    0: MSE = 1.5809\n",
      "Epoch 1000: MSE = 0.7069\n",
      "Epoch 2000: MSE = 0.0347\n",
      "Epoch 3000: MSE = 0.0082\n",
      "Epoch 4000: MSE = 0.0042\n",
      "Epoch 5000: MSE = 0.0027\n",
      "Epoch 6000: MSE = 0.0020\n",
      "Epoch 7000: MSE = 0.0016\n",
      "Epoch 8000: MSE = 0.0013\n",
      "Epoch 9000: MSE = 0.0011\n",
      "0 ^ 0 = 0 [0]\n",
      "0 ^ 1 = 1 [1]\n",
      "1 ^ 0 = 1 [1]\n",
      "1 ^ 1 = 0 [0]\n",
      "final mse: 0.0009374897713963145\n"
     ]
    }
   ],
   "source": [
    "n_loops = 10000\n",
    "\n",
    "# train network with sigmoid\n",
    "nn = XOR_net(activation=\"sigmoid\")\n",
    "for i in range(n_loops):\n",
    "    nn.update_weights(0.1)\n",
    "    if not i % (n_loops // 10):\n",
    "        print(f\"Epoch {i:{int(np.log10(n_loops))}}: MSE = {nn.mse():.4f}\")\n",
    "nn.print_test()\n",
    "\n",
    "# train network with tanh\n",
    "nn = XOR_net(activation=\"tanh\")\n",
    "for i in range(n_loops):\n",
    "    nn.update_weights(0.01)\n",
    "    if not i % (n_loops // 10):\n",
    "        print(f\"Epoch {i:{int(np.log10(n_loops))}}: MSE = {nn.mse():.4f}\")\n",
    "nn.print_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
