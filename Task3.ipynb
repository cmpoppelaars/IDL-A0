{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a class for the XOR_net\n",
    "class XOR_net:\n",
    "    def __init__(self, activation=\"sigmoid\", weights=None):\n",
    "        FUNC_TABLE = {\"sigmoid\": self.sigmoid, \"tanh\": self.tanh}\n",
    "        DFUNC_TABLE = {\"sigmoid\": self.dsigmoid, \"tanh\": self.dtanh}\n",
    "        self.weights = self.random_weights() if weights is None else weights\n",
    "        # Storage for skipping the calculation of some variables\n",
    "        # 0, 1: inputs of NN\n",
    "        # 2, 3: values of the hidden layer pre activation\n",
    "        # 4, 5: values of the hidden layer post activation\n",
    "        # 6: value of the final node pre activation\n",
    "        # 7: value of the final node post activation\n",
    "        self.store = np.zeros(8)\n",
    "        self.actf = FUNC_TABLE[activation]  # activation function\n",
    "        self.dactf = DFUNC_TABLE[activation]  # derivative of activation function\n",
    "        self.inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        self.outputs = np.array([0, 1, 1, 0])\n",
    "\n",
    "    def simulate(self, inpt):\n",
    "        self.store[:2] = inpt\n",
    "        self.store[2:4] = np.dot(self.weights[:6].reshape((2, 3)), np.append(inpt, 1))\n",
    "        self.store[4:6] = [self.actf(el) for el in self.store[2:4]]\n",
    "        self.store[6] = np.dot(self.weights[6:], np.append(self.store[4:6], 1))\n",
    "        self.store[7] = self.actf(self.store[6])\n",
    "        return self\n",
    "\n",
    "    def output(self, discrete=False):\n",
    "        if discrete:\n",
    "            return 1 if self.store[-1] >= 0.5 else 0\n",
    "        else:\n",
    "            return self.store[-1]\n",
    "\n",
    "    def mse(self):\n",
    "        return sum(\n",
    "            [\n",
    "                (self.simulate(inpt).output() - output) ** 2\n",
    "                for inpt, output in zip(self.inputs, self.outputs)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def grdmse(self):\n",
    "        par_derivs = np.zeros((9))\n",
    "        for inpt, output in zip(self.inputs, self.outputs):\n",
    "            self.simulate(inpt)\n",
    "            temp = 2 * (self.store[-1] - output) * self.dactf(self.store[6])\n",
    "\n",
    "            par_derivs[6:] += temp * np.append(self.store[4:6], 1)\n",
    "            par_derivs[3:6] += (\n",
    "                temp\n",
    "                * self.weights[7]\n",
    "                * self.dactf(self.store[3])\n",
    "                * np.append(self.store[:2], 1)\n",
    "            )\n",
    "            par_derivs[:3] += (\n",
    "                temp\n",
    "                * self.weights[6]\n",
    "                * self.dactf(self.store[2])\n",
    "                * np.append(self.store[:2], 1)\n",
    "            )\n",
    "        return par_derivs\n",
    "\n",
    "    def update_weights(self, eta=0.01):\n",
    "        self.weights -= eta * self.grdmse()\n",
    "\n",
    "    def random_weights(self):\n",
    "        weights = np.zeros((9))\n",
    "        weights[:6] = np.random.normal(0, np.sqrt(2 / (2 + 2)), (6))\n",
    "        weights[6:] = np.random.normal(0, np.sqrt(2 / (2 + 1)), (3))\n",
    "        return weights\n",
    "\n",
    "    def print_test(self, discrete=True):\n",
    "        for inpt, output in zip(self.inputs, self.outputs):\n",
    "            print(\n",
    "                f\"{inpt[0]} ^ {inpt[1]} = {self.simulate(inpt).output(discrete):{'.3f' if not discrete else ''}} [{output}]\"\n",
    "            )\n",
    "        print(f\"final mse: {self.mse()}\")\n",
    "\n",
    "    def is_xor(self):\n",
    "        for inpt, output in zip(self.inputs, self.outputs):\n",
    "            if self.simulate(inpt).output(True) != output:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @lru_cache()\n",
    "    def sigmoid(self, x):\n",
    "        return (1 + np.exp(-x)) ** -1\n",
    "\n",
    "    @lru_cache()\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    @lru_cache()\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @lru_cache()\n",
    "    def dtanh(self, x):\n",
    "        return np.cosh(x) ** -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ^ 0 = 0.499 [0]\n",
      "0 ^ 1 = 0.501 [1]\n",
      "1 ^ 0 = 0.500 [1]\n",
      "1 ^ 1 = 0.498 [0]\n",
      "final mse: 0.9950899883152184\n",
      "0 ^ 0 = 0.500 [0]\n",
      "0 ^ 1 = 0.500 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.496 [0]\n",
      "final mse: 0.9948604407992582\n",
      "0 ^ 0 = 0.498 [0]\n",
      "0 ^ 1 = 0.501 [1]\n",
      "1 ^ 0 = 0.507 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.989545655150563\n",
      "0 ^ 0 = 0.492 [0]\n",
      "0 ^ 1 = 0.501 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.493 [0]\n",
      "final mse: 0.9841403372946582\n",
      "0 ^ 0 = 0.495 [0]\n",
      "0 ^ 1 = 0.504 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.9898046120870893\n",
      "0 ^ 0 = 0.495 [0]\n",
      "0 ^ 1 = 0.501 [1]\n",
      "1 ^ 0 = 0.536 [1]\n",
      "1 ^ 1 = 0.493 [0]\n",
      "final mse: 0.9524022513419405\n",
      "0 ^ 0 = 0.499 [0]\n",
      "0 ^ 1 = 0.503 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.498 [0]\n",
      "final mse: 0.9933433834765888\n",
      "0 ^ 0 = 0.494 [0]\n",
      "0 ^ 1 = 0.508 [1]\n",
      "1 ^ 0 = 0.504 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.9808198989890589\n",
      "0 ^ 0 = 0.494 [0]\n",
      "0 ^ 1 = 0.502 [1]\n",
      "1 ^ 0 = 0.505 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.9859503841468101\n",
      "0 ^ 0 = 0.498 [0]\n",
      "0 ^ 1 = 0.504 [1]\n",
      "1 ^ 0 = 0.501 [1]\n",
      "1 ^ 1 = 0.499 [0]\n",
      "final mse: 0.9922193678482171\n",
      "Average number of iterations: 797587\n"
     ]
    }
   ],
   "source": [
    "n_total = []\n",
    "for i in range(10):\n",
    "    satisfied = False\n",
    "    n = 0\n",
    "    while not satisfied:\n",
    "        n += 1\n",
    "        nn = XOR_net()\n",
    "        satisfied = nn.is_xor()\n",
    "    nn.print_test(False)\n",
    "    n_total.append(n)\n",
    "print(f\"Average number of iterations: {int(np.mean(n_total))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: MSE = 1.0184\n",
      "Epoch 1000: MSE = 0.9954\n",
      "Epoch 2000: MSE = 0.8066\n",
      "Epoch 3000: MSE = 0.1216\n",
      "Epoch 4000: MSE = 0.0273\n",
      "Epoch 5000: MSE = 0.0143\n",
      "Epoch 6000: MSE = 0.0095\n",
      "Epoch 7000: MSE = 0.0071\n",
      "Epoch 8000: MSE = 0.0056\n",
      "Epoch 9000: MSE = 0.0046\n",
      "0 ^ 0 = 0 [0]\n",
      "0 ^ 1 = 1 [1]\n",
      "1 ^ 0 = 1 [1]\n",
      "1 ^ 1 = 0 [0]\n",
      "final mse: 0.003947755317323755\n",
      "Epoch    0: MSE = 7.9028\n",
      "Epoch 1000: MSE = 0.9388\n",
      "Epoch 2000: MSE = 0.5347\n",
      "Epoch 3000: MSE = 0.0152\n",
      "Epoch 4000: MSE = 0.0056\n",
      "Epoch 5000: MSE = 0.0032\n",
      "Epoch 6000: MSE = 0.0023\n",
      "Epoch 7000: MSE = 0.0017\n",
      "Epoch 8000: MSE = 0.0014\n",
      "Epoch 9000: MSE = 0.0011\n",
      "0 ^ 0 = 0 [0]\n",
      "0 ^ 1 = 1 [1]\n",
      "1 ^ 0 = 1 [1]\n",
      "1 ^ 1 = 0 [0]\n",
      "final mse: 0.000977537431950747\n"
     ]
    }
   ],
   "source": [
    "n_loops = 10000\n",
    "\n",
    "# train network with sigmoid\n",
    "nn = XOR_net(activation=\"sigmoid\")\n",
    "for i in range(n_loops):\n",
    "    nn.update_weights(0.1)\n",
    "    if not i % (n_loops // 10):\n",
    "        print(f\"Epoch {i:{int(np.log10(n_loops))}}: MSE = {nn.mse():.4f}\")\n",
    "nn.print_test()\n",
    "\n",
    "# train network with tanh\n",
    "nn = XOR_net(activation=\"tanh\")\n",
    "for i in range(n_loops):\n",
    "    nn.update_weights(0.01)\n",
    "    if not i % (n_loops // 10):\n",
    "        print(f\"Epoch {i:{int(np.log10(n_loops))}}: MSE = {nn.mse():.4f}\")\n",
    "nn.print_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
