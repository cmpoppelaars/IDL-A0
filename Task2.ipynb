{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from scipy.special import softmax as sf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "PATH = os.getcwd()\n",
    "DATAPATH = os.path.join(PATH, \"data\")\n",
    "filenames = {\n",
    "    \"X_test\": \"test_in - Copy.csv\",\n",
    "    \"X_train\": \"train_in - Copy.csv\",\n",
    "    \"y_test\": \"test_out - Copy.csv\",\n",
    "    \"y_train\": \"train_out - Copy.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# Import all data files\n",
    "X_train = pd.read_csv(os.path.join(DATAPATH, filenames[\"X_train\"]), header=None)\n",
    "y_train = pd.read_csv(\n",
    "    os.path.join(DATAPATH, filenames[\"y_train\"]), header=None, names=[\"digit\"]\n",
    ")\n",
    "expected_train = pd.get_dummies(y_train.digit).to_numpy()\n",
    "\n",
    "X_test = pd.read_csv(os.path.join(DATAPATH, filenames[\"X_test\"]), header=None)\n",
    "y_test = pd.read_csv(\n",
    "    os.path.join(DATAPATH, filenames[\"y_test\"]), header=None, names=[\"digit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 122.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2379.494837290569\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 117.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2380.3194221282038\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [00:01<00:07, 112.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timvd\\Documents\\Uni 2023-2024\\IDL\\IDL-A0\\Task2.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     weights,acc_test,err \u001b[39m=\u001b[39m train_perceptron(n_loops\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, small_batch\u001b[39m=\u001b[39;49msmall_batch, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     accs\u001b[39m.\u001b[39mappend(acc_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     errs\u001b[39m.\u001b[39mappend(err)\n",
      "\u001b[1;32mc:\\Users\\timvd\\Documents\\Uni 2023-2024\\IDL\\IDL-A0\\Task2.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         weights \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m eta \u001b[39m*\u001b[39m calc_gradient(weights, X\u001b[39m.\u001b[39miloc[idxs], expected[idxs])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         weights \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m eta \u001b[39m*\u001b[39m calc_gradient(weights, X, expected)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m err \u001b[39m=\u001b[39m calc_error(weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m acc_train \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(classify(weights, X_train) \u001b[39m==\u001b[39m y_train\u001b[39m.\u001b[39mdigit) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y_train\u001b[39m.\u001b[39mdigit)\n",
      "\u001b[1;32mc:\\Users\\timvd\\Documents\\Uni 2023-2024\\IDL\\IDL-A0\\Task2.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalc_gradient\u001b[39m(weights, X\u001b[39m=\u001b[39mX_train, expected\u001b[39m=\u001b[39mexpected_train, actf\u001b[39m=\u001b[39msoftmax):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     s \u001b[39m=\u001b[39m actf(np\u001b[39m.\u001b[39mdot(append_one(np\u001b[39m.\u001b[39;49marray(X))\u001b[39m.\u001b[39mT, weights))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     y \u001b[39m=\u001b[39m expected\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timvd/Documents/Uni%202023-2024/IDL/IDL-A0/Task2.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(append_one(np\u001b[39m.\u001b[39marray(X)), (s \u001b[39m-\u001b[39m y))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We use scipy's softmax, due to issues with NaN's\n",
    "def softmax(x):\n",
    "    return sf(x, axis=1)\n",
    "\n",
    "\n",
    "def random_weights_gauss(shape, sigma):\n",
    "    return np.random.normal(0, sigma, shape)\n",
    "\n",
    "\n",
    "def append_one(X, axis=0):\n",
    "    shape = (X.shape[axis], 1) if axis == 1 else (1, X.shape[axis])\n",
    "    return np.append(X.T, np.ones(shape=shape), axis=axis)\n",
    "\n",
    "\n",
    "def classify(weights, X=X_test, actf=softmax):\n",
    "    y_out = actf(np.dot(append_one(X).T, weights))\n",
    "    return y_out.argmax(axis=1)\n",
    "\n",
    "\n",
    "def calc_error(weights, X=X_test, y=y_test, actf=softmax):\n",
    "    output = actf(np.dot(append_one(X).T, weights))\n",
    "    rows = list(np.arange(output.shape[0]))\n",
    "    return -np.log10(output)[rows, y.digit].sum()\n",
    "\n",
    "\n",
    "def calc_gradient(weights, X=X_train, expected=expected_train, actf=softmax):\n",
    "    s = actf(np.dot(append_one(np.array(X)).T, weights))\n",
    "    y = expected\n",
    "    return np.dot(append_one(np.array(X)), (s - y))\n",
    "\n",
    "\n",
    "def train_perceptron(\n",
    "    n_loops=1000, eta=0.01, X=X_train, y=y_train, small_batch=False, batch_size=10\n",
    "):\n",
    "    weights = random_weights_gauss((257, 10), np.sqrt(2 / (256 + 10)))\n",
    "    expected = pd.get_dummies(y.digit).to_numpy()\n",
    "\n",
    "    for epoch in tqdm(range(n_loops)):\n",
    "        if small_batch:\n",
    "            # only use a small batch of the input and expected\n",
    "            idxs = np.random.choice(X.shape[0], batch_size)\n",
    "            weights -= eta * calc_gradient(weights, X.iloc[idxs], expected[idxs])\n",
    "        else:\n",
    "            weights -= eta * calc_gradient(weights, X, expected)\n",
    "\n",
    "    err = calc_error(weights)\n",
    "    acc_train = sum(classify(weights, X_train) == y_train.digit) / len(y_train.digit)\n",
    "    acc_test = sum(classify(weights, X_test) == y_test.digit) / len(y_test.digit)\n",
    "    print(f\"Final error: {err}\")\n",
    "    print(\n",
    "        f\"Final accuraccy:\\n\\tOn training data: {acc_train:.3f}\\n\\tOn testing data: {acc_test:.3f}\"\n",
    "    )\n",
    "    return weights, acc_test, err\n",
    "\n",
    "small_batch = False\n",
    "accs = []\n",
    "errs = []\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    weights,acc_test,err = train_perceptron(n_loops=1000, small_batch=small_batch, batch_size=100)\n",
    "    accs.append(acc_test)\n",
    "    errs.append(err)\n",
    "print(f\"Mean accuracy on test set after {n} runs is {np.mean(accs):.3f}\")\n",
    "print(f\"Mean error on test set after {n} runs is {np.mean(errs):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
