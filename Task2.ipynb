{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from scipy.special import softmax as sf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "PATH = os.getcwd()\n",
    "DATAPATH = os.path.join(PATH, \"data\")\n",
    "filenames = {\n",
    "    \"X_test\": \"test_in - Copy.csv\",\n",
    "    \"X_train\": \"train_in - Copy.csv\",\n",
    "    \"y_test\": \"test_out - Copy.csv\",\n",
    "    \"y_train\": \"train_out - Copy.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# Import all data files\n",
    "X_train = pd.read_csv(os.path.join(DATAPATH, filenames[\"X_train\"]), header=None)\n",
    "y_train = pd.read_csv(\n",
    "    os.path.join(DATAPATH, filenames[\"y_train\"]), header=None, names=[\"digit\"]\n",
    ")\n",
    "expected_train = pd.get_dummies(y_train.digit).to_numpy()\n",
    "\n",
    "X_test = pd.read_csv(os.path.join(DATAPATH, filenames[\"X_test\"]), header=None)\n",
    "y_test = pd.read_csv(\n",
    "    os.path.join(DATAPATH, filenames[\"y_test\"]), header=None, names=[\"digit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 120.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2204.4883442832142\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 126.18it/s]\n",
      "C:\\Users\\timvd\\AppData\\Local\\Temp\\ipykernel_25184\\745302691.py:23: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(output)[rows, y.digit].sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2702.8276022929936\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 133.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2602.8094060857593\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 133.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2507.5230501941896\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 134.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2261.650484468889\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 133.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2479.6768624282377\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 138.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2601.3446798419027\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 141.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2298.6651861042033\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 139.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2403.786611617801\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 132.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2346.06219571209\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.870\n",
      "Mean accuracy on test set after 10 runs is 0.868\n",
      "Mean error on test set after 10 runs is 2440.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We use scipy's softmax, due to issues with NaN's\n",
    "def softmax(x):\n",
    "    return sf(x, axis=1)\n",
    "\n",
    "\n",
    "def random_weights_gauss(shape, sigma):\n",
    "    return np.random.normal(0, sigma, shape)\n",
    "\n",
    "\n",
    "def append_one(X, axis=0):\n",
    "    shape = (X.shape[axis], 1) if axis == 1 else (1, X.shape[axis])\n",
    "    return np.append(X.T, np.ones(shape=shape), axis=axis)\n",
    "\n",
    "\n",
    "def classify(weights, X=X_test, actf=softmax):\n",
    "    y_out = actf(np.dot(append_one(X).T, weights))\n",
    "    return y_out.argmax(axis=1)\n",
    "\n",
    "\n",
    "def calc_error(weights, X=X_test, y=y_test, actf=softmax):\n",
    "    output = actf(np.dot(append_one(X).T, weights))\n",
    "    rows = list(np.arange(output.shape[0]))\n",
    "    return -np.log10(output)[rows, y.digit].sum()\n",
    "\n",
    "\n",
    "def calc_gradient(weights, X=X_train, expected=expected_train, actf=softmax):\n",
    "    s = actf(np.dot(append_one(np.array(X)).T, weights))\n",
    "    y = expected\n",
    "    return np.dot(append_one(np.array(X)), (s - y))\n",
    "\n",
    "\n",
    "def train_perceptron(\n",
    "    n_loops=1000, eta=0.01, X=X_train, y=y_train, small_batch=False, batch_size=10\n",
    "):\n",
    "    weights = random_weights_gauss((257, 10), np.sqrt(2 / (256 + 10)))\n",
    "    expected = pd.get_dummies(y.digit).to_numpy()\n",
    "\n",
    "    for epoch in tqdm(range(n_loops)):\n",
    "        if small_batch:\n",
    "            # only use a small batch of the input and expected\n",
    "            idxs = np.random.choice(X.shape[0], batch_size)\n",
    "            weights -= eta * calc_gradient(weights, X.iloc[idxs], expected[idxs])\n",
    "        else:\n",
    "            weights -= eta * calc_gradient(weights, X, expected)\n",
    "\n",
    "    err = calc_error(weights)\n",
    "    acc_train = sum(classify(weights, X_train) == y_train.digit) / len(y_train.digit)\n",
    "    acc_test = sum(classify(weights, X_test) == y_test.digit) / len(y_test.digit)\n",
    "    print(f\"Final error: {err}\")\n",
    "    print(\n",
    "        f\"Final accuraccy:\\n\\tOn training data: {acc_train:.3f}\\n\\tOn testing data: {acc_test:.3f}\"\n",
    "    )\n",
    "    return weights, acc_test, err\n",
    "\n",
    "small_batch = False\n",
    "accs = []\n",
    "errs = []\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    weights,acc_test,err = train_perceptron(n_loops=1000, small_batch=small_batch, batch_size=100)\n",
    "    accs.append(acc_test)\n",
    "    errs.append(err)\n",
    "print(f\"Mean accuracy on test set after {n} runs is {np.mean(accs):.3f}\")\n",
    "print(f\"Mean error on test set after {n} runs is {np.mean(errs):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
