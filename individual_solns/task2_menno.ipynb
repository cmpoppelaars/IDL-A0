{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "DATAPATH = os.path.join(PATH, \"../data\")\n",
    "filenames = {\n",
    "    \"X_test\": \"test_in - Copy.csv\",\n",
    "    \"X_train\": \"train_in - Copy.csv\",\n",
    "    \"y_test\": \"test_out - Copy.csv\",\n",
    "    \"y_train\": \"train_out - Copy.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# Import all data files\n",
    "X_train = pd.read_csv(os.path.join(DATAPATH, filenames[\"X_train\"]), header=None)\n",
    "y_train = pd.read_csv(\n",
    "    os.path.join(DATAPATH, filenames[\"y_train\"]), header=None, names=[\"digit\"]\n",
    ")\n",
    "X_test = pd.read_csv(os.path.join(DATAPATH, filenames[\"X_test\"]), header=None)\n",
    "y_test = pd.read_csv(\n",
    "    os.path.join(DATAPATH, filenames[\"y_test\"]), header=None, names=[\"digit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax as sf\n",
    "def softmax(x): return sf(x, axis=1)\n",
    "\n",
    "\n",
    "def random_weights_gauss(shape, sigma):\n",
    "    return np.random.normal(0, sigma, shape)\n",
    "\n",
    "def random_weights(shape, bounds=[-1, 1]):\n",
    "    return np.random.uniform(*bounds, size=shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:00<00:25, 39.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 55.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error: 2775.0372462466903\n",
      "Final accuraccy:\n",
      "\tOn training data: 1.000\n",
      "\tOn testing data: 0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\menno\\AppData\\Local\\Temp\\ipykernel_14496\\4003262799.py:12: RuntimeWarning: divide by zero encountered in log10\n",
      "  return -np.log10(output)[rows, y.digit].sum()\n"
     ]
    }
   ],
   "source": [
    "def append_one(X, axis=0):\n",
    "    shape = (X.shape[axis], 1) if axis==1 else (1, X.shape[axis])\n",
    "    return np.append(X.T, np.ones(shape=shape), axis=axis)\n",
    "\n",
    "def classify(weights, X=X_test, actf=softmax):\n",
    "    y_out = actf(np.dot(append_one(X).T, weights))\n",
    "    return y_out.argmax(axis=1)\n",
    "\n",
    "def calc_error(weights, X=X_test, y=y_test, actf=softmax):\n",
    "    output = actf(np.dot(append_one(X).T, weights))\n",
    "    rows = list(np.arange(output.shape[0]))\n",
    "    return -np.log10(output)[rows, y.digit].sum()\n",
    "\n",
    "expected = pd.get_dummies(y_train.digit).to_numpy()\n",
    "def calc_gradient(weights, X=X_train, expected=expected, actf=softmax):\n",
    "    s = actf(np.dot(append_one(np.array(X)).T, weights))\n",
    "    y = expected\n",
    "    return np.dot(append_one(np.array(X)), (s-y))\n",
    "\n",
    "def train_perceptron(n_loops = 1000, eta = 0.01, X = X_train, y=y_train, small_batch = False, batch_size=10):\n",
    "    weights = random_weights_gauss((257, 10), np.sqrt(2/(256+10)))\n",
    "    expected = pd.get_dummies(y.digit).to_numpy()\n",
    "\n",
    "    for epoch in tqdm(range(n_loops)):\n",
    "        if small_batch:\n",
    "            # only use a small batch of the input and expected\n",
    "            idxs = np.random.choice(X.shape[0], batch_size)\n",
    "            weights -= eta * calc_gradient(weights, X.iloc[idxs], expected[idxs])\n",
    "        else:\n",
    "            weights -= eta * calc_gradient(weights, X, expected)\n",
    "\n",
    "    acc_train = sum(classify(weights, X_train) == y_train.digit)/len(y_train.digit)\n",
    "    acc_test = sum(classify(weights, X_test) == y_test.digit)/len(y_test.digit)\n",
    "    print(f\"Final error: {calc_error(weights)}\")\n",
    "    print(f\"Final accuraccy:\\n\\tOn training data: {acc_train:.3f}\\n\\tOn testing data: {acc_test:.3f}\")\n",
    "    return weights\n",
    "\n",
    "weights = train_perceptron(n_loops=1000) #, small_batch=True, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
